import torch
import comfy.model_management as mm
from comfy.model_patcher import ModelPatcher

class LaoLi_Lineup_Node:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                # 1. æ˜¾å­˜é˜ˆå€¼: è¶…è¿‡æ­¤æ¯”ä¾‹(0.85)è§¦å‘æ¸…ç†
                "vram_threshold": ("FLOAT", {
                    "default": 0.85, 
                    "min": 0.1, 
                    "max": 1.0, 
                    "step": 0.05,
                    "display": "number"
                }),
                # 2. æ¸…ç†é—´éš”: æ¯ N å±‚æ¸…ç†ä¸€æ¬¡
                "cleaning_interval": ("INT", {
                    "default": 1, 
                    "min": 1, 
                    "max": 10, 
                    "step": 1,
                    "display": "number"
                }),
                # 3. ä¸¥æ ¼æ¨¡å¼: True=åŒæ­¥+æ¸…ç†, False=ä»…æ¸…ç†
                "strict_mode": ("BOOLEAN", {
                    "default": True
                }),
            }
        }

    RETURN_TYPES = ("MODEL",)
    RETURN_NAMES = ("optimized_model",)
    FUNCTION = "apply_lineup"
    CATEGORY = "LaoLi Nodes/Optimization"
    # è¿™é‡Œæ˜¯é¼ æ ‡æ‚¬åœåœ¨èŠ‚ç‚¹æ ‡é¢˜ä¸Šæ—¶ä¼šæ˜¾ç¤ºçš„è¯´æ˜
    DESCRIPTION = "è€ææ˜¾å­˜æ’é˜Ÿ V9:\n- vram_threshold: æ˜¾å­˜å ç”¨è¶…è¿‡æ­¤æ¯”ä¾‹(é»˜è®¤0.85)æ—¶è§¦å‘æ¸…ç†ã€‚\n- cleaning_interval: æ¸…ç†é¢‘ç‡(é»˜è®¤1ï¼Œå³æ¯å±‚éƒ½åˆ¤æ–­)ã€‚\n- strict_mode: å¼€å¯é˜²å´©(åŒæ­¥+æ¸…ç†)ï¼Œå…³é—­æé€Ÿ(ä»…æ¸…ç†)ã€‚"

    def apply_lineup(self, model, vram_threshold, cleaning_interval, strict_mode):
        # 1. å®‰å…¨æ£€æŸ¥
        if not isinstance(model, ModelPatcher):
            return (model,)

        try:
            new_model = model.clone()
            
            # 2. è·å–å½“å‰ä½¿ç”¨çš„ GPU è®¾å¤‡
            device = mm.get_torch_device()
            
            # åªæœ‰åœ¨ GPU æ¨¡å¼ä¸‹æ‰å¯ç”¨æ˜¾å­˜ç›‘æ§
            total_vram = 0
            if device.type == 'cuda':
                # è·å–å½“å‰è®¾å¤‡çš„æ€»æ˜¾å­˜
                total_vram = torch.cuda.get_device_properties(device).total_memory
            
            # --- å®šä¹‰æ™ºèƒ½é’©å­ ---
            def smart_hook(module, input):
                # å¦‚æœä¸æ˜¯ CUDA è®¾å¤‡ï¼Œç›´æ¥è·³è¿‡
                if total_vram == 0: 
                    return None

                # A. æ˜¾å­˜ç›‘æ§
                # memory_reserved æ˜¯ PyTorch å‘ç³»ç»Ÿç”³è¯·çš„æ˜¾å­˜ï¼Œmemory_allocated æ˜¯å®é™…å ç”¨çš„
                # æˆ‘ä»¬ä½¿ç”¨ reserved æ¥åˆ¤æ–­æ˜¯å¦æ¥è¿‘ç‰©ç†æé™
                current_reserved = torch.cuda.memory_reserved(device)
                usage_ratio = current_reserved / total_vram

                # B. é˜ˆå€¼åˆ¤æ–­ (é»˜è®¤ > 85%)
                if usage_ratio >= vram_threshold:
                    if strict_mode:
                        torch.cuda.synchronize() # å¼ºåˆ¶ GPU åœæœºç­‰å¾… (é˜²å´©å…³é”®)
                    mm.soft_empty_cache()       # é‡Šæ”¾æœªé”å®šæ˜¾å­˜
                
                return None

            # 3. æŒ‚è½½é€»è¾‘
            blocks = self._find_blocks(new_model.model)
            mounted_count = 0
            
            for i, block in enumerate(blocks):
                # éµå®ˆé—´éš”è®¾å®š (é€šå¸¸è®¾ä¸º1ï¼Œå³æ¯å±‚éƒ½ç›‘æ§)
                if i % cleaning_interval == 0:
                    block.register_forward_pre_hook(smart_hook)
                    mounted_count += 1

            # æ§åˆ¶å°è¾“å‡ºç¡®è®¤ä¿¡æ¯
            print(f"ğŸš€ [è€æ Lineup V9] å¯åŠ¨ | è®¾å¤‡: {device} | é˜ˆå€¼: {int(vram_threshold*100)}% | æ¨¡å¼: {'ä¸¥æ ¼(åŒæ­¥)' if strict_mode else 'æé€Ÿ(å¼‚æ­¥)'}")
            
            return (new_model,)

        except Exception as e:
            print(f"âŒ [LaoLi_Lineup Error] {e}")
            return (model,)

    def _find_blocks(self, module):
        """é€’å½’æŸ¥æ‰¾æ¨¡å‹ä¸­çš„è®¡ç®—å±‚"""
        blocks = []
        target_names = [
            'transformer_blocks', 'double_blocks', 'single_blocks', 
            'blocks', 'input_blocks', 'middle_block', 'output_blocks'
        ]
        
        # ä¼˜å…ˆæŸ¥æ‰¾åº•å±‚ diffusion_model
        root = getattr(module, 'diffusion_model', module)

        # æµ…å±‚æœç´¢
        for name in target_names:
            attr = getattr(root, name, None)
            if isinstance(attr, (list, torch.nn.ModuleList)):
                blocks.extend(attr)
        
        # æ·±å±‚æœç´¢ (é˜²æ­¢æ¼ç½‘ä¹‹é±¼)
        if not blocks:
            for name, child in root.named_children():
                if any(t in name for t in target_names):
                    if isinstance(child, (list, torch.nn.ModuleList)):
                        blocks.extend(child)
        
        return blocks

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "LaoLi_Lineup": LaoLi_Lineup_Node
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LaoLi_Lineup": "è€æ_LaoLiğŸš€ Lineup (æ˜¾å­˜æ’é˜Ÿ)"
}